{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOm1mmqOj9hzkpAWNKEaWb0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7UkYKV4scZKq","colab_type":"code","colab":{}},"source":["from builtins import print\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","\n","matplotlib.rcParams['font.family'] = 'sans-serif'\n","matplotlib.rcParams['font.sans-serif'] = 'Arial'\n","import os\n","import operator\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.preprocessing import LabelEncoder\n","\n","from scipy.interpolate import interp1d\n","from scipy.io import loadmat\n","\n","\n","def calculate_metrics(y_true, y_pred, duration, y_true_val=None, y_pred_val=None):\n","    res = pd.DataFrame(data=np.zeros((1, 4), dtype=np.float), index=[0],\n","                       columns=['precision', 'accuracy', 'recall', 'duration'])\n","    res['precision'] = precision_score(y_true, y_pred, average='macro')\n","    res['accuracy'] = accuracy_score(y_true, y_pred)\n","\n","    if not y_true_val is None:\n","        # this is useful when transfer learning is used with cross validation\n","        res['accuracy_val'] = accuracy_score(y_true_val, y_pred_val)\n","\n","    res['recall'] = recall_score(y_true, y_pred, average='macro')\n","    res['duration'] = duration\n","    return res\n","\n","def save_logs(output_directory, hist, y_pred, y_true, duration, lr=True, y_true_val=None, y_pred_val=None):\n","    hist_df = pd.DataFrame(hist.history)\n","    hist_df.to_csv(output_directory + 'history.csv', index=False)\n","\n","    df_metrics = calculate_metrics(y_true, y_pred, duration, y_true_val, y_pred_val)\n","    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n","\n","    index_best_model = hist_df['loss'].idxmin()\n","    row_best_model = hist_df.loc[index_best_model]\n","\n","    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float), index=[0],\n","                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n","                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n","\n","    df_best_model['best_model_train_loss'] = row_best_model['loss']\n","    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n","    df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n","    df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n","    if lr == True:\n","        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n","    df_best_model['best_model_nb_epoch'] = index_best_model\n","\n","    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n","\n","    # for FCN there is no hyperparameters fine tuning - everything is static in code\n","\n","    # plot losses\n","    plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n","\n","    return df_metrics"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82CJvwE9cCQY","colab_type":"code","colab":{}},"source":["# MLP model \n","import tensorflow.keras as keras\n","import tensorflow as tf\n","import numpy as np\n","import time\n","\n","import matplotlib \n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt \n","\n","\n","class Classifier_MLP:\n","\n","\tdef __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True):\n","\t\tself.output_directory = output_directory\n","\t\tif build == True:\n","\t\t\tself.model = self.build_model(input_shape, nb_classes)\n","\t\t\tif(verbose==True):\n","\t\t\t\tself.model.summary()\n","\t\t\tself.verbose = verbose\n","\t\t\tself.model.save_weights(self.output_directory + 'model_init.hdf5')\n","\t\treturn\n","\n","\tdef build_model(self, input_shape, nb_classes):\n","\t\tinput_layer = keras.layers.Input(input_shape)\n","\n","\t\t# flatten/reshape because when multivariate all should be on the same axis \n","\t\tinput_layer_flattened = keras.layers.Flatten()(input_layer)\n","\t\t\n","\t\tlayer_1 = keras.layers.Dropout(0.1)(input_layer_flattened)\n","\t\tlayer_1 = keras.layers.Dense(500, activation='relu')(layer_1)\n","\n","\t\tlayer_2 = keras.layers.Dropout(0.2)(layer_1)\n","\t\tlayer_2 = keras.layers.Dense(500, activation='relu')(layer_2)\n","\n","\t\tlayer_3 = keras.layers.Dropout(0.2)(layer_2)\n","\t\tlayer_3 = keras.layers.Dense(500, activation='relu')(layer_3)\n","\n","\t\toutput_layer = keras.layers.Dropout(0.3)(layer_3)\n","\t\toutput_layer = keras.layers.Dense(nb_classes, activation='softmax')(output_layer)\n","\n","\t\tmodel = keras.models.Model(inputs=input_layer, outputs=output_layer)\n","              #多类的对数损失\n","\t\tmodel.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adadelta(),\n","\t\t\tmetrics=['accuracy'])\n","\n","\t\treduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=200, min_lr=0.1)\n","\n","\t\tfile_path = self.output_directory+'best_model.hdf5' \n","\n","\t\tmodel_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n","\t\t\tsave_best_only=True)\n","\n","\t\tself.callbacks = [reduce_lr,model_checkpoint]\n","\n","\t\treturn model\n","\n","\tdef fit(self, x_train, y_train, x_val, y_val,y_true):\n","\t\tif not tf.test.is_gpu_available:\n","\t\t\tprint('error')\n","\t\t\texit()\n","\t\t# x_val and y_val are only used to monitor the test loss and NOT for training  \n","\t\tbatch_size = 16\n","\t\tnb_epochs = 5000\n","\n","\t\tmini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n","\n","\t\tstart_time = time.time() \n","\n","\t\thist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n","\t\t\tverbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n","\t\t\n","\t\tduration = time.time() - start_time\n","\n","\t\tself.model.save(self.output_directory + 'last_model.hdf5')\n","\n","\t\tmodel = keras.models.load_model(self.output_directory+'best_model.hdf5')\n","\n","\t\ty_pred = model.predict(x_val)\n","\n","\t\t# convert the predicted from binary to integer \n","\t\ty_pred = np.argmax(y_pred , axis=1)\n","\n","\t\tsave_logs(self.output_directory, hist, y_pred, y_true, duration)\n","\n","\t\tkeras.backend.clear_session()\n","\n","\tdef predict(self, x_test, y_true,x_train,y_train,y_test,return_df_metrics = True):\n","\t\tmodel_path = self.output_directory + 'best_model.hdf5'\n","\t\tmodel = keras.models.load_model(model_path)\n","\t\ty_pred = model.predict(x_test)\n","\t\tif return_df_metrics:\n","\t\t\ty_pred = np.argmax(y_pred, axis=1)\n","\t\t\tdf_metrics = calculate_metrics(y_true, y_pred, 0.0)\n","\t\t\treturn df_metrics\n","\t\telse:\n","\t\t\treturn y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPAH5FWfknXX","colab_type":"code","colab":{}},"source":["#定义的单变量UCR数据集的名字，这里只使用了21个数据集，如果想对所有数据进行实验请在这里进行自动补充\n","DATASET_NAMES_2018 = ['ACSF1', 'Adiac', 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken', 'BME', 'Car', 'CBF', 'Chinatown',\n","                      'CinCECGTorso', 'Coffee', 'Computers', 'CricketX',\n","                      'CricketY', 'CricketZ', 'DiatomSizeReduction',\n","                      'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW',\n","                      'Earthquakes']\n","#定义多变量UCR数据集，这里只使用了12个，若果想对所有数据集进行实验请在这里进行补充\n","MTS_DATASET_NAMES = ['ArabicDigits', 'AUSLAN', 'CharacterTrajectories', 'CMUsubject16', 'ECG',\n","                    'JapaneseVowels', 'KickvsPunch', 'Libras', 'NetFlow', 'UWave', 'Wafer', 'WalkvsRun']\n","\n","dataset_names_for_archive = {'UCRArchive_2018': DATASET_NAMES_2018}\n","ARCHIVE_NAMES = ['UCRArchive_2018']\n","ITERATIONS = 5  # nb of random runs for random initializations\n","CLASSIFIERS = ['fcn', 'mlp', 'resnet', 'tlenet', 'mcnn', 'twiesn', 'encoder', 'mcdcnn', 'cnn', 'inception']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8wiNu4UnN82","colab_type":"code","colab":{}},"source":["#这里定义读取三种数据集的方式，一种是读取单变量的UCR数据集，一种是读取多变量的UCR数据集，还有一种是读取一般的不属于UCR的数据集\n","def read_all_datasets(root_dir, archive_name, split_val=False):\n","    datasets_dict = {}\n","    dataset_names_to_sort = []\n","    cur_root_dir = root_dir.replace('-temp', '')\n","    if archive_name == 'mts_archive':\n","\n","        for dataset_name in MTS_DATASET_NAMES:\n","            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n","\n","            x_train = np.load(root_dir_dataset + 'x_train.npy')\n","            y_train = np.load(root_dir_dataset + 'y_train.npy')\n","            x_test = np.load(root_dir_dataset + 'x_test.npy')\n","            y_test = np.load(root_dir_dataset + 'y_test.npy')\n","\n","            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                           y_test.copy())\n","    elif archive_name == 'UCRArchive_2018':\n","        for dataset_name in DATASET_NAMES_2018:\n","            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name \n","\n","            df_train = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TRAIN.tsv', sep='\\t', header=None)\n","\n","            df_test = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TEST.tsv', sep='\\t', header=None)\n","\n","            y_train = df_train.values[:, 0]\n","            y_test = df_test.values[:, 0]\n","\n","            x_train = df_train.drop(columns=[0])\n","            x_test = df_test.drop(columns=[0])\n","\n","            x_train.columns = range(x_train.shape[1])\n","            x_test.columns = range(x_test.shape[1])\n","\n","            x_train = x_train.values\n","            x_test = x_test.values\n","\n","            # znorm\n","            std_ = x_train.std(axis=1, keepdims=True)\n","            std_[std_ == 0] = 1.0\n","            x_train = (x_train - x_train.mean(axis=1, keepdims=True)) / std_\n","\n","            std_ = x_test.std(axis=1, keepdims=True)\n","            std_[std_ == 0] = 1.0\n","            x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n","\n","            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                           y_test.copy())\n","\n","    else:\n","        for dataset_name in DATASET_NAMES:\n","            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n","            file_name = root_dir_dataset + dataset_name\n","            x_train, y_train = readucr(file_name + '_TRAIN')\n","            x_test, y_test = readucr(file_name + '_TEST')\n","\n","            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                           y_test.copy())\n","\n","            dataset_names_to_sort.append((dataset_name, len(x_train)))\n","\n","        dataset_names_to_sort.sort(key=operator.itemgetter(1))\n","\n","        for i in range(len(DATASET_NAMES)):\n","            DATASET_NAMES[i] = dataset_names_to_sort[i][0]\n","\n","    return datasets_dict\n","#创建路径\n","def create_directory(directory_path):\n","    if os.path.exists(directory_path):\n","        return None\n","    else:\n","        try:\n","            os.makedirs(directory_path)\n","        except:\n","            # in case another machine created the path meanwhile !:(\n","            return None\n","        return directory_path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sA-it_RWbiOV","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import sys\n","import sklearn\n","\n","#这里可以添加其他方法\n","def create_classifier(input_shape, nb_classes, output_directory, verbose=False):\n","\n","    return Classifier_MLP(output_directory, input_shape, nb_classes, verbose)\n","\n","def fit_classifier():\n","    x_train = datasets_dict[dataset_name][0]\n","    y_train = datasets_dict[dataset_name][1]\n","    x_test = datasets_dict[dataset_name][2]\n","    y_test = datasets_dict[dataset_name][3]\n","\n","    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n","\n","    # transform the labels from integers to one hot vectors\n","    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n","    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n","    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n","    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n","\n","    # save orignal y because later we will use binary\n","    y_true = np.argmax(y_test, axis=1)\n","\n","    if len(x_train.shape) == 2:  # if univariate\n","        # add a dimension to make it multivariate with one dimension \n","        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n","        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n","\n","    input_shape = x_train.shape[1:]\n","    classifier = create_classifier(input_shape, nb_classes, output_directory)\n","\n","    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"69NjyleTwBJ0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"outputId":"3003f8dc-ab49-4829-c318-a6080dc29f0d","executionInfo":{"status":"ok","timestamp":1585559498247,"user_tz":-480,"elapsed":65667,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ck7SO9LTwPHU","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNsm6FFEw1MD","colab_type":"code","colab":{}},"source":["dir = '/content/gdrive/My Drive/时序数据分类/DL/archives/UCRArchive_2018/ACSF1/ACSF1_TRAIN.tsv'\n","a = pd.read_csv(dir, sep='\\t', header=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTR-7oWYx1Af","colab_type":"code","colab":{}},"source":["def plot_epochs_metric(hist, file_name, metric='loss'):\n","    plt.figure()\n","    plt.plot(hist.history[metric])\n","    plt.plot(hist.history['val_' + metric])\n","    plt.title('model ' + metric)\n","    plt.ylabel(metric, fontsize='large')\n","    plt.xlabel('epoch', fontsize='large')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.savefig(file_name, bbox_inches='tight')\n","    plt.close()\n","\n","def save_logs(output_directory, hist, y_pred, y_true, duration, lr=True, y_true_val=None, y_pred_val=None):\n","    hist_df = pd.DataFrame(hist.history)\n","    hist_df.to_csv(output_directory + 'history.csv', index=False)\n","\n","    df_metrics = calculate_metrics(y_true, y_pred, duration, y_true_val, y_pred_val)\n","    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n","\n","    index_best_model = hist_df['loss'].idxmin()\n","    row_best_model = hist_df.loc[index_best_model]\n","\n","    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float), index=[0],\n","                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n","                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n","\n","    df_best_model['best_model_train_loss'] = row_best_model['loss']\n","    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n","    df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n","    df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n","    if lr == True:\n","        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n","    df_best_model['best_model_nb_epoch'] = index_best_model\n","\n","    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n","\n","    # for FCN there is no hyperparameters fine tuning - everything is static in code\n","\n","    # plot losses\n","    plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n","\n","    return df_metrics"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVHCeUE-v-k0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":613},"outputId":"65e1e44e-9571-43e6-d499-9179e638f499"},"source":["############################################### main\n","\n","# change this directory for your machine\n","root_dir = '/content/gdrive/My Drive/时序数据分类/DL/'\n","\n","for archive_name in ARCHIVE_NAMES:\n","  print('\\tarchive_name', archive_name)\n","\n","  datasets_dict = read_all_datasets(root_dir, archive_name)\n","\n","  for iter in range(ITERATIONS):\n","    print('\\t\\titer', iter)\n","\n","    trr = ''\n","    if iter != 0:\n","      trr = '_itr_' + str(iter)\n","      tmp_output_directory = root_dir + '/results/' + 'MLP' + '/' + archive_name + trr + '/'\n","\n","      for dataset_name in dataset_names_for_archive[archive_name]:\n","\n","        print('\\t\\t\\tdataset_name: ', dataset_name)\n","\n","        output_directory = tmp_output_directory + dataset_name + '/'\n","\n","        create_directory(output_directory)\n","\n","        fit_classifier()\n","\n","        print('\\t\\t\\t\\tDONE')\n","\n","        # the creation of this directory means\n","        create_directory(output_directory + '/DONE')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\tarchive_name UCRArchive_2018\n","\t\titer 0\n","\t\titer 1\n","\t\t\tdataset_name:  ACSF1\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"],"name":"stdout"},{"output_type":"stream","text":["findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n","findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"],"name":"stderr"},{"output_type":"stream","text":["\t\t\t\tDONE\n","\t\t\tdataset_name:  Adiac\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["\t\t\t\tDONE\n","\t\t\tdataset_name:  ArrowHead\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","\t\t\t\tDONE\n","\t\t\tdataset_name:  Beef\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","\t\t\t\tDONE\n","\t\t\tdataset_name:  BeetleFly\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","\t\t\t\tDONE\n","\t\t\tdataset_name:  BirdChicken\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","\t\t\t\tDONE\n","\t\t\tdataset_name:  BME\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","\t\t\t\tDONE\n","\t\t\tdataset_name:  Car\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","\t\t\t\tDONE\n","\t\t\tdataset_name:  CBF\n"],"name":"stdout"}]}]}